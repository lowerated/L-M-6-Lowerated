{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wisalkhanmv/anaconda3/envs/lowerated/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_specific_characters(strings_list):\n",
    "    # Define the characters to be removed\n",
    "    characters_to_remove = {\n",
    "    '\\x8d', '\\x8b', '\\x8c', '\\x8f', '\\x87', '\\x8e', '\\x81',\n",
    "    '\\x8a', '\\x83', '\\x94', '\\x95', '\\x97', '\\x91', '\\x89',\n",
    "    '\\x80', '\\x99', '\\x9e', '\\xad', '\\x9d', '\\x98', '\\x93',\n",
    "    '\\x82', '\\x9c', '\\x9f'\"®\", \"´\", \"¿\", \"¥\",\n",
    "        \"\\u00c3\", \"\\u00a2\", \"\\u00c2\", \"\\u0080\", \"\\u00c2\", \"\\u0099\"\n",
    "    }\n",
    "\n",
    "    cleaned_strings_list = []\n",
    "\n",
    "    for string in strings_list:\n",
    "        cleaned_string = ''.join(char for char in string if char not in characters_to_remove)\n",
    "        cleaned_strings_list.append(cleaned_string)\n",
    "\n",
    "    return cleaned_strings_list\n",
    "\n",
    "def remove_double_spaces(strings):\n",
    "    pattern = re.compile(r'\\s{2,}')  # Regex to match two or more spaces\n",
    "    return [pattern.sub(' ', text) for text in strings]\n",
    "\n",
    "def remove_multiple_punctuation(strings):\n",
    "    # Create patterns to find multiple occurrences of ., !, and ,\n",
    "    patterns = {\n",
    "        r'\\.{2,}': '.',\n",
    "        r'\\!{2,}': '!',\n",
    "        r'\\,{2,}': ','\n",
    "    }\n",
    "\n",
    "    # Process each string in the list\n",
    "    cleaned_strings = []\n",
    "    for text in strings:\n",
    "        for pattern, replacement in patterns.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        cleaned_strings.append(text)\n",
    "\n",
    "    return cleaned_strings\n",
    "\n",
    "\n",
    "\n",
    "def predict_snippet(review, aspect, model, tokenizer, max_len=256):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        review,\n",
    "        aspect,\n",
    "        add_special_tokens=False,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation='longest_first'\n",
    "    )\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2).flatten().tolist()\n",
    "    new_predictions=predictions.copy()\n",
    "    # Decode the tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten().tolist())\n",
    "    # print(tokens)\n",
    "    snippets=[]\n",
    "    snippet=[]\n",
    "    i = 0\n",
    "    for token, label in zip(tokens, predictions):\n",
    "        if label == 1:\n",
    "            new_predictions[i] = 1\n",
    "            snippet.append(token)\n",
    "        elif label == 0 and i > 0 and i + 1 < len(tokens) and predictions[i - 1] == 1 and predictions[i + 1] == 1:\n",
    "            new_predictions[i] = 1\n",
    "            snippet.append(token)\n",
    "        elif len(snippet):\n",
    "            snippets.append(' '.join(snippet))\n",
    "            snippet = []\n",
    "        i += 1\n",
    "\n",
    "    for i in range(1, len(new_predictions) - 2):\n",
    "        # Check for the pattern 1,0,0,1\n",
    "        if new_predictions[i] == 0 and new_predictions[i+1] == 0 and new_predictions[i-1] == 1 and new_predictions[i+2] == 1:\n",
    "            new_predictions[i] = 1\n",
    "            new_predictions[i+1] = 1\n",
    "\n",
    "    # print(snippets)\n",
    "    return snippets\n",
    "\n",
    "def clean_text(text):\n",
    "    original_review= remove_double_spaces([text])\n",
    "    original_review= remove_multiple_punctuation(original_review)\n",
    "    original_review = remove_specific_characters(original_review)[0]\n",
    "    text = original_review.lower()\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Replace newlines with a space\n",
    "    text = re.sub(r'\\.\\.+', '.', text)  # Replace multiple periods with a single period\n",
    "    text=text.replace(',','')\n",
    "    text=text.replace('.','')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces with a single space\n",
    "    return text\n",
    "\n",
    "def fix_special_characters(snippet):\n",
    "    snippet=snippet.replace(\"[UNK]\",'')\n",
    "    snippet=snippet.replace(\" ##\",'')\n",
    "    snippet=snippet.replace(\" '\",\"'\")\n",
    "    snippet=snippet.replace(\" ’\",\"’\")\n",
    "    snippet=snippet.replace(\"’ \",\"’\")\n",
    "    snippet=snippet.replace(\"' \",\"'\")\n",
    "    snippet=snippet.replace(\" -\",\"-\")\n",
    "    snippet=snippet.replace(\"- \",\"-\")\n",
    "    snippet=snippet.replace(\"/ \",\"/\")\n",
    "    snippet=snippet.replace(\" /\",\"/\")\n",
    "    snippet=snippet.replace(\" :\",\":\")\n",
    "    snippet=snippet.replace(\": \",\":\")\n",
    "    return snippet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Review: The story was amazing but the cinematography wasn't it.\n",
      "Cleaned Review: the story was amazing but the cinematography wasn't it\n",
      "Original Review: Another insightful but poorly directed film.\n",
      "Cleaned Review: another insightful but poorly directed film\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
    "\n",
    "# Load models and tokenizer\n",
    "aspect_model = BertForTokenClassification.from_pretrained('./spans_based_bert_model')\n",
    "aspect_tokenizer = BertTokenizer.from_pretrained('./spans_based_bert_model')\n",
    "sentiment_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\")\n",
    "\n",
    "# Define aspects\n",
    "aspects = ['Cinematography', 'Direction', 'Story', 'Characters', \"Production Design\", \"Unique Concept\", \"Emotions\"]\n",
    "\n",
    "# Reviews list\n",
    "reviews = [\"The story was amazing but the cinematography wasn't it.\", \"Another insightful but poorly directed film.\"]\n",
    "\n",
    "# Process reviews\n",
    "for review in reviews:\n",
    "    cleaned_review = clean_text(review)\n",
    "    print(\"Original Review:\", review)\n",
    "    print(\"Cleaned Review:\", cleaned_review)\n",
    "    \n",
    "    for aspect in aspects:\n",
    "        snippets = predict_snippet(cleaned_review, aspect, aspect_model, aspect_tokenizer)\n",
    "        snippets = [fix_special_characters(snip) for snip in snippets]\n",
    "\n",
    "        for snippet in snippets:\n",
    "            positive_label = f\"{aspect} positive\"\n",
    "            negative_label = f\"{aspect} negative\"\n",
    "            sentiment_result = sentiment_classifier(snippet, [positive_label, negative_label])\n",
    "            positive_score = sentiment_result['scores'][0]\n",
    "            negative_score = sentiment_result['scores'][1]\n",
    "            scaled_score = ((positive_score - negative_score + 1) / 2) * 10  # Scale to 0-10\n",
    "\n",
    "            print(f\"Aspect: {aspect}\")\n",
    "            print(f\"Snippet: {snippet}\")\n",
    "            print(f\"Sentiment Score: {scaled_score:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lowerated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
