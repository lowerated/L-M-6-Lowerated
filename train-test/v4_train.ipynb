{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "with open(\"new_spans_labeled.json\", 'r') as file:\n",
        "    dataset = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Function to check if a snippet exists within a review and adjust the snippet length if necessary.\n",
        "def find_snippet(review, snippet):\n",
        "    \"\"\"Check for exact or near-exact matches of a snippet in a review text.\n",
        "    \n",
        "    Args:\n",
        "        review (str): The review text.\n",
        "        snippet (str): The snippet to find in the review.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (bool, str) indicating if found, and the matched snippet (possibly adjusted).\n",
        "    \"\"\"\n",
        "    if snippet in review:\n",
        "        return True, snippet\n",
        "    elif snippet[:-1] in review:\n",
        "        return True, snippet[:-1]\n",
        "    return False, snippet\n",
        "\n",
        "# Process each review and identify reviews containing all specified snippets from aspects.\n",
        "def process_reviews(dataset, aspects):\n",
        "    \"\"\"Process a dataset of reviews to find and adjust snippets related to specified aspects.\n",
        "    \n",
        "    Args:\n",
        "        dataset (list of dict): The dataset containing review details.\n",
        "        aspects (list of str): List of aspects to check in each review.\n",
        "    \n",
        "    Returns:\n",
        "        list: Filtered list of reviews containing all specified snippets.\n",
        "    \"\"\"\n",
        "    reviews_list = []\n",
        "    for data in dataset:\n",
        "        review_text = data['review'].translate(str.maketrans({\"'\": '', '\"': '', '“': '', ']': ''})).lower().strip()\n",
        "        found = True\n",
        "\n",
        "        for aspect in aspects:\n",
        "            for i, item in enumerate(data.get(aspect, [])):\n",
        "                snippet = item.translate(str.maketrans({\"'\": '', '\"': '', '“': '', ']': ''})).lower().strip()\n",
        "                is_found, adjusted_snippet = find_snippet(review_text, snippet)\n",
        "                if not is_found:\n",
        "                    found = False\n",
        "                    break\n",
        "                else:\n",
        "                    data[aspect][i] = adjusted_snippet\n",
        "            if not found:\n",
        "                break\n",
        "\n",
        "        if found:\n",
        "            reviews_list.append(data)\n",
        "    \n",
        "    return reviews_list\n",
        "\n",
        "# Create a DataFrame from processed review data.\n",
        "def create_dataframe(reviews_list, aspects):\n",
        "    \"\"\"Generate a DataFrame from review data with encoded aspects and snippets.\n",
        "    \n",
        "    Args:\n",
        "        reviews_list (list of dict): The processed list of reviews.\n",
        "        aspects (list of str): Aspects to encode.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame: A DataFrame with the review, aspect, snippets, and encoded aspect index.\n",
        "    \"\"\"\n",
        "    aspect_encoding = {aspect: idx for idx, aspect in enumerate(aspects)}\n",
        "    data = []\n",
        "\n",
        "    for review in reviews_list:\n",
        "        review_text = review['review']\n",
        "        for aspect in aspects:\n",
        "            snippets = review.get(aspect, [])\n",
        "            snippets = snippets if snippets else ['']\n",
        "            entry = {\n",
        "                \"review\": review_text,\n",
        "                \"aspect\": aspect,\n",
        "                \"snippets\": snippets,\n",
        "                \"aspect_encoded\": aspect_encoding[aspect]\n",
        "            }\n",
        "            data.append(entry)\n",
        "    \n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Example usage\n",
        "aspects = ['Cinematography', 'Direction', 'Story', 'Characters', \"Production Design\",\"Emotions\",\"Unique Concept\"]\n",
        "reviews_list = process_reviews(dataset, aspects)\n",
        "df = create_dataframe(reviews_list, aspects)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Dictionary to hold counts\n",
        "label_counts = {\n",
        "    'Cinematography': 0,\n",
        "    'Direction': 0,\n",
        "    'Story': 0,\n",
        "    'Characters': 0,\n",
        "    'Production Design': 0,\n",
        "    \"Emotions\":0,\n",
        "    \"Unique Concept\":0\n",
        "}\n",
        "\n",
        "# Counting the number of elements for each label in each review\n",
        "for review in reviews_list:\n",
        "    for label in label_counts:\n",
        "        if len(review[label]):\n",
        "            # print(review[label])\n",
        "            label_counts[label] += 1\n",
        "\n",
        "# Printing the counts\n",
        "print(\"Label counts:\")\n",
        "for label, count in label_counts.items():\n",
        "    print(f\"{label}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN6EL6G-DCs_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T48YD2gsJkT2",
        "outputId": "c8b56159-6b5f-4984-852c-a6e65fcd5775"
      },
      "outputs": [],
      "source": [
        "# Define the dataset class\n",
        "class ReviewAspectDataset(Dataset):\n",
        "    def __init__(self, reviews, aspects, snippets, tokenizer, max_len):\n",
        "        self.reviews = reviews\n",
        "        self.aspects = aspects\n",
        "        self.snippets = snippets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = str(self.reviews[idx])\n",
        "        aspect = str(self.aspects[idx])\n",
        "        snippets = self.snippets[idx]\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            aspect,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        tokens = self.tokenizer.tokenize(review)\n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        labels = [0] * len(token_ids)\n",
        "\n",
        "        for snippet in snippets:\n",
        "            snippet_tokens = self.tokenizer.tokenize(snippet)\n",
        "            snippet_token_ids = self.tokenizer.convert_tokens_to_ids(snippet_tokens)\n",
        "\n",
        "            for i in range(len(token_ids) - len(snippet_token_ids) + 1):\n",
        "                if token_ids[i:i+len(snippet_token_ids)] == snippet_token_ids:\n",
        "                    labels[i:i+len(snippet_token_ids)] = [1] * len(snippet_token_ids)\n",
        "                    break  # Assuming one occurrence of snippet in review\n",
        "\n",
        "        # Pad or truncate labels to match max_len\n",
        "        if len(labels) < self.max_len:\n",
        "            labels += [0] * (self.max_len - len(labels))\n",
        "        else:\n",
        "            labels = labels[:self.max_len]\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.long)\n",
        "        }\n",
        "# Tokenizer and dataset preparation\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_len = 512\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaQ1FWHnJl9N"
      },
      "outputs": [],
      "source": [
        "train_dataset = ReviewAspectDataset(\n",
        "    reviews=train_df['review'].to_numpy(),\n",
        "    aspects=train_df['aspect'].to_numpy(),\n",
        "    snippets=train_df['snippets'].to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        ")\n",
        "\n",
        "val_dataset = ReviewAspectDataset(\n",
        "    reviews=val_df['review'].to_numpy(),\n",
        "    aspects=val_df['aspect'].to_numpy(),\n",
        "    snippets=val_df['snippets'].to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPrBikRGJsr8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import EarlyStoppingCallback\n",
        "import numpy as np\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "# Disable wandb logging\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "\n",
        "# Load a BERT model pre-trained on 'bert-base-uncased' for token classification with two labels\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "\n",
        "# Define a custom loss function that adjusts class weights based on specific tokens\n",
        "def custom_loss_fn(outputs, labels, token):\n",
        "    # Assign initial class weights to prioritize tokens labeled as 1 over those labeled as 0.\n",
        "    class_weights = torch.tensor([2.520658379555701, 9.601626815691288], device=\"cuda\")\n",
        "    # Adjust weights for specific classes\n",
        "    if token in [\"cinematography\", \"design\", \"direction\",\"emotions\",\"concept\"]:\n",
        "        class_weights[0] += 2.4562\n",
        "        class_weights[1] += 4.4562\n",
        "\n",
        "    # Use the CrossEntropyLoss with dynamic class weights\n",
        "    loss_fct = CrossEntropyLoss(weight=class_weights)\n",
        "    active_loss = labels.view(-1) != -100  # Filter out the padding tokens for loss calculation\n",
        "    active_logits = outputs.view(-1, model.num_labels)[active_loss]\n",
        "    active_labels = labels.view(-1)[active_loss]\n",
        "    loss = loss_fct(active_logits, active_labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Define metrics computation for evaluation using accuracy and weighted F1 score\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=2)\n",
        "\n",
        "    # Flatten the predictions and labels for metric calculation\n",
        "    preds_flat = preds.flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    # Calculate accuracy and F1 score\n",
        "    accuracy = accuracy_score(labels_flat, preds_flat)\n",
        "    f1 = f1_score(labels_flat, preds_flat, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "# Set training arguments for the Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',  # Directory for saving output files\n",
        "    num_train_epochs=4,  # Number of training epochs\n",
        "    per_device_train_batch_size=52,  # Batch size for training\n",
        "    per_device_eval_batch_size=52,  # Batch size for evaluation\n",
        "    warmup_steps=500,  # Number of warmup steps\n",
        "    weight_decay=0.01,  # Weight decay for regularization\n",
        "    logging_dir='./logs',  # Directory for storing logs\n",
        "    logging_steps=500,  # Log every 500 steps\n",
        "    evaluation_strategy=\"steps\",  # Evaluate every 500 steps\n",
        "    eval_steps=500,\n",
        "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
        "    save_total_limit=1,  # Save only the best model to limit disk usage\n",
        "    learning_rate=5e-05  # Learning rate\n",
        ")\n",
        "\n",
        "# Define a custom trainer class that uses the custom loss function\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].flatten())\n",
        "        index = tokens.index(\"[PAD]\")  # Find the padding token\n",
        "        token = tokens[index - 2]  # Get the last real token before padding\n",
        "\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = custom_loss_fn(logits, labels, token)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Initialize the trainer with custom configurations\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=8)]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_YOZxWxJ0vN"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.disable(logging.WARNING)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmqSX7JfgqY8"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvl_85K4gx0H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "# Custom evaluation function to generate classification report\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=2)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    # Flatten the predictions and labels\n",
        "    preds_flat = preds.flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    # Filter out the labels where the label is -100 (ignore index)\n",
        "    mask = labels_flat != -100\n",
        "    preds_flat = preds_flat[mask]\n",
        "    labels_flat = labels_flat[mask]\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(labels_flat, preds_flat, target_names=['O', 'B-SNIPPET'], digits=4)\n",
        "    accuracy = accuracy_score(labels_flat, preds_flat)\n",
        "\n",
        "    print(\"\\nClassification Report:\\n\", report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Safely extract metrics from the report\n",
        "    report_lines = report.split('\\n')\n",
        "    f1_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    \n",
        "    # Extract metrics from each class, skipping the header and footer lines\n",
        "    for line in report_lines[2:-3]:\n",
        "        parts = line.split()\n",
        "        if len(parts) >= 4:  # Ensure that the line has enough parts\n",
        "            recall_scores.append(float(parts[-4]))\n",
        "            precision_scores.append(float(parts[-3]))\n",
        "            f1_scores.append(float(parts[-2]))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": np.mean(f1_scores) if f1_scores else 0.0,\n",
        "        \"precision\": np.mean(precision_scores) if precision_scores else 0.0,\n",
        "        \"recall\": np.mean(recall_scores) if recall_scores else 0.0\n",
        "    }\n",
        "\n",
        "# Trainer with custom metrics\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics  # Adding the custom compute_metrics function\n",
        ")\n",
        "\n",
        "# Evaluate the model on validation dataset\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vSEwLXfeHZ2"
      },
      "source": [
        "\n",
        "# Save the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_pretrained('./review_spans_extraction')\n",
        "tokenizer.save_pretrained('./review_spans_extraction')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2Ram0_fpeIkC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "\n",
        "import re\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = BertForTokenClassification.from_pretrained('./review_spans_extraction')\n",
        "tokenizer = BertTokenizer.from_pretrained('./review_spans_extraction')\n",
        "\n",
        "def remove_specific_characters(strings_list):\n",
        "    # Define the characters to be removed\n",
        "    characters_to_remove = {\n",
        "    '\\x8d', '\\x8b', '\\x8c', '\\x8f', '\\x87', '\\x8e', '\\x81',\n",
        "    '\\x8a', '\\x83', '\\x94', '\\x95', '\\x97', '\\x91', '\\x89',\n",
        "    '\\x80', '\\x99', '\\x9e', '\\xad', '\\x9d', '\\x98', '\\x93',\n",
        "    '\\x82', '\\x9c', '\\x9f'\"®\", \"´\", \"¿\", \"¥\",\n",
        "        \"\\u00c3\", \"\\u00a2\", \"\\u00c2\", \"\\u0080\", \"\\u00c2\", \"\\u0099\"\n",
        "    }\n",
        "\n",
        "    cleaned_strings_list = []\n",
        "\n",
        "    for string in strings_list:\n",
        "        cleaned_string = ''.join(char for char in string if char not in characters_to_remove)\n",
        "        cleaned_strings_list.append(cleaned_string)\n",
        "\n",
        "    return cleaned_strings_list\n",
        "\n",
        "def remove_double_spaces(strings):\n",
        "    pattern = re.compile(r'\\s{2,}')  # Regex to match two or more spaces\n",
        "    return [pattern.sub(' ', text) for text in strings]\n",
        "\n",
        "def remove_multiple_punctuation(strings):\n",
        "    # Create patterns to find multiple occurrences of ., !, and ,\n",
        "    patterns = {\n",
        "        r'\\.{2,}': '.',\n",
        "        r'\\!{2,}': '!',\n",
        "        r'\\,{2,}': ','\n",
        "    }\n",
        "\n",
        "    # Process each string in the list\n",
        "    cleaned_strings = []\n",
        "    for text in strings:\n",
        "        for pattern, replacement in patterns.items():\n",
        "            text = re.sub(pattern, replacement, text)\n",
        "        cleaned_strings.append(text)\n",
        "\n",
        "    return cleaned_strings\n",
        "\n",
        "\n",
        "\n",
        "def predict_snippet(review, aspect, model, tokenizer, max_len=512):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        review,\n",
        "        aspect,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_len,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation='longest_first'\n",
        "    )\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Make predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=2).flatten().tolist()\n",
        "    new_predictions=predictions.copy()\n",
        "    # Decode the tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten().tolist())\n",
        "    # print(tokens)\n",
        "    snippets=[]\n",
        "    snippet=[]\n",
        "    i = 0\n",
        "    for token, label in zip(tokens, predictions):\n",
        "        if label == 1:\n",
        "            new_predictions[i] = 1\n",
        "            snippet.append(token)\n",
        "        elif label == 0 and i > 0 and i + 1 < len(tokens) and predictions[i - 1] == 1 and predictions[i + 1] == 1:\n",
        "            new_predictions[i] = 1\n",
        "            snippet.append(token)\n",
        "        elif len(snippet):\n",
        "            snippets.append(' '.join(snippet))\n",
        "            snippet = []\n",
        "        i += 1\n",
        "    for i in range(1, len(new_predictions) - 2):\n",
        "        # Check for the pattern 1,0,0,1\n",
        "        if new_predictions[i] == 0 and new_predictions[i+1] == 0 and new_predictions[i-1] == 1 and new_predictions[i+2] == 1:\n",
        "            new_predictions[i] = 1\n",
        "            new_predictions[i+1] = 1\n",
        "\n",
        "    # print(snippets)\n",
        "    return snippets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rj072SAUeNFi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original review:  \"this movie really surprised me. i had my doubts about it at first but the movie got better and better for each minute. it is maybe not for the action seeking audience but for those that like an explicit portrait of a very strange criminal, man, lover and husband. if you're not a fan of bad language or sexual content this really is not for you. <br /><br />the storyline is somewhat hard to follow sometimes, but in the end i think it made everything better. the ending was unexpected since you were almost fouled to think it would end otherwise. <br /><br />as for the acting i think it was good. it will not be up for an oscar award for long but it at least caught my eye. gil bellows portrait of a prison man is not always perfect but it is very entertaining. shaun parkes portrait of bellows prison mate clinique is great and extremely powerful. on the downside i think i will put esai morales portrait of markie. take my advice and watch this movie\n",
            "Cinematography []\n",
            "\n",
            "-------------------\n",
            "Direction []\n",
            "\n",
            "-------------------\n",
            "Story ['> the storyline is somewhat hard to follow sometimes , but in the end i think it made everything better . the ending was unexpected since you were almost fouled to think it would end']\n",
            "\n",
            "-------------------\n",
            "Characters ['> as for the acting i think it was good .', '. gil bellows portrait of a prison man is not always perfect but it is very entertaining . shaun parkes portrait of bellows prison mate clinique is great and extremely powerful . on the downside i think i will put esai morales portrait of markie']\n",
            "\n",
            "-------------------\n",
            "Production Design []\n",
            "\n",
            "-------------------\n",
            "Emotions []\n",
            "\n",
            "-------------------\n",
            "Unique Concept ['. it is maybe not for the action seeking audience but for those that like an explicit portrait of a very strange criminal , man , lover and husband']\n",
            "\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    original_review= remove_double_spaces([text])\n",
        "    original_review= remove_multiple_punctuation(original_review)\n",
        "    original_review = remove_specific_characters(original_review)[0]\n",
        "    text = original_review.lower()\n",
        "    text = re.sub(r'\\n+', ' ', text)  # Replace newlines with a space\n",
        "    text = re.sub(r'\\.\\.+', '.', text)  # Replace multiple periods with a single period\n",
        "    # text=text.replace(',','')\n",
        "    # text=text.replace('.','')\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces with a single space\n",
        "    return text\n",
        "\n",
        "def fix_special_characters(snippet):\n",
        "    snippet=snippet.replace(\"[UNK]\",'')\n",
        "    snippet=snippet.replace(\" ##\",'')\n",
        "    snippet=snippet.replace(\" '\",\"'\")\n",
        "    snippet=snippet.replace(\" ’\",\"’\")\n",
        "    snippet=snippet.replace(\"’ \",\"’\")\n",
        "    snippet=snippet.replace(\"' \",\"'\")\n",
        "    snippet=snippet.replace(\" -\",\"-\")\n",
        "    snippet=snippet.replace(\"- \",\"-\")\n",
        "    snippet=snippet.replace(\"/ \",\"/\")\n",
        "    snippet=snippet.replace(\" /\",\"/\")\n",
        "    snippet=snippet.replace(\" :\",\":\")\n",
        "    snippet=snippet.replace(\": \",\":\")\n",
        "    return snippet\n",
        "\n",
        "aspects = [\"Cinematography\", \"Direction\", \"Story\", \"Characters\", \"Production Design\",\"Emotions\",\"Unique Concept\"]\n",
        "original_review = \"\"\"\"This movie really surprised me. \n",
        "I had my doubts about it at first but the movie got better and better for each minute. \n",
        "It is maybe not for the action seeking audience but for those that like an explicit portrait of a very strange criminal, man, lover and husband. \n",
        "If you're not a fan of bad language or sexual content this really is not for you. <br /><br />The storyline is somewhat hard to follow sometimes, but in the end I think it made everything better. \n",
        "The ending was unexpected since you were almost fouled to think it would end otherwise. <br /><br />As for the acting I think it was good.\n",
        "It will not be up for an Oscar award for long but it at least caught my eye. Gil Bellows portrait of a prison man is not always perfect but it is very entertaining. \n",
        "Shaun Parkes portrait of Bellows prison mate Clinique is great and extremely powerful. On the downside I think I will put Esai Morales portrait of Markie.\n",
        "Take my advice and watch this movie\"\"\"\n",
        "original_review=clean_text(original_review)\n",
        "\n",
        "print(\"original review: \",original_review)\n",
        "\n",
        "for aspect in aspects:\n",
        "    predicted_snippets = predict_snippet(original_review, aspect, model, tokenizer)\n",
        "    new_snippets = []\n",
        "    for snippet in predicted_snippets:\n",
        "        new_snippets.append(fix_special_characters(snippet))\n",
        "\n",
        "    print(aspect, new_snippets, end='\\n')\n",
        "    print(\"\\n-------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
