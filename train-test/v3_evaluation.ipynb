{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wisalkhanmv/anaconda3/envs/lowerated/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForTokenClassification.from_pretrained('./model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./model')\n",
    "\n",
    "def remove_specific_characters(strings_list):\n",
    "    # Define the characters to be removed\n",
    "    characters_to_remove = {\n",
    "    '\\x8d', '\\x8b', '\\x8c', '\\x8f', '\\x87', '\\x8e', '\\x81', \n",
    "    '\\x8a', '\\x83', '\\x94', '\\x95', '\\x97', '\\x91', '\\x89', \n",
    "    '\\x80', '\\x99', '\\x9e', '\\xad', '\\x9d', '\\x98', '\\x93' , \n",
    "    '\\x82', '\\x9c', '\\x9f'\"®\", \"´\", \"¿\", \"¥\",\n",
    "        \"\\u00c3\", \"\\u00a2\", \"\\u00c2\", \"\\u0080\", \"\\u00c2\", \"\\u0099\"\n",
    "    }\n",
    "    \n",
    "    cleaned_strings_list = []\n",
    "    \n",
    "    for string in strings_list:\n",
    "        cleaned_string = ''.join(char for char in string if char not in characters_to_remove)\n",
    "        cleaned_strings_list.append(cleaned_string)\n",
    "    \n",
    "    return cleaned_strings_list\n",
    "\n",
    "def remove_double_spaces(strings):\n",
    "    pattern = re.compile(r'\\s{2,}')  # Regex to match two or more spaces\n",
    "    return [pattern.sub(' ', text) for text in strings]\n",
    "\n",
    "def remove_multiple_punctuation(strings):\n",
    "    # Create patterns to find multiple occurrences of ., !, and ,\n",
    "    patterns = {\n",
    "        r'\\.{2,}': '.',  \n",
    "        r'\\!{2,}': '!',  \n",
    "        r'\\,{2,}': ','   \n",
    "    }\n",
    "\n",
    "    # Process each string in the list\n",
    "    cleaned_strings = []\n",
    "    for text in strings:\n",
    "        for pattern, replacement in patterns.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        cleaned_strings.append(text)\n",
    "    \n",
    "    return cleaned_strings\n",
    "\n",
    "def fix_special_characters(snippet):\n",
    "    snippet=snippet.replace(\"[UNK]\",'')\n",
    "    snippet=snippet.replace(\" ##\",'')\n",
    "    snippet=snippet.replace(\" '\",\"'\")\n",
    "    snippet=snippet.replace(\" ’\",\"’\")\n",
    "    snippet=snippet.replace(\"’ \",\"’\")\n",
    "    snippet=snippet.replace(\"' \",\"'\")\n",
    "    snippet=snippet.replace(\" -\",\"-\")\n",
    "    snippet=snippet.replace(\"- \",\"-\")\n",
    "    snippet=snippet.replace(\"/ \",\"/\")\n",
    "    snippet=snippet.replace(\" /\",\"/\")\n",
    "    snippet=snippet.replace(\" :\",\":\")\n",
    "    snippet=snippet.replace(\": \",\":\")\n",
    "    return snippet\n",
    "\n",
    "\n",
    "def predict_snippet(review, aspect, model, tokenizer, max_len=512):\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        review,\n",
    "        aspect,\n",
    "        add_special_tokens=False,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation='longest_first'\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(\"cuda\")\n",
    "    attention_mask = inputs['attention_mask'].to(\"cuda\")\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2).flatten().tolist()\n",
    "    new_predictions=predictions.copy()\n",
    "    # Decode the tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten().tolist())\n",
    "    \n",
    "    i = 0\n",
    "    for token, label in zip(tokens, predictions):\n",
    "        if label == 1:\n",
    "            new_predictions[i] = 1\n",
    "        elif label == 0 and i > 0 and i + 1 < len(tokens) and predictions[i - 1] == 1 and predictions[i + 1] == 1:\n",
    "            new_predictions[i] = 1\n",
    "        i += 1\n",
    "\n",
    "    for i in range(1, len(new_predictions) - 2):\n",
    "        # Check for the pattern 1,0,0,1\n",
    "        if new_predictions[i] == 0 and new_predictions[i+1] == 0 and new_predictions[i-1] == 1 and new_predictions[i+2] == 1:\n",
    "            new_predictions[i] = 1\n",
    "            new_predictions[i+1] = 1\n",
    "    i = 0\n",
    "    snippets=[]\n",
    "    snippet=[]\n",
    "    for token, label in zip(tokens, new_predictions):\n",
    "        if label == 1:\n",
    "            snippet.append(token)\n",
    "        elif len(snippet):\n",
    "            res=' '.join(snippet)\n",
    "            res=fix_special_characters(res)\n",
    "            snippets.append(res)\n",
    "            snippet = []\n",
    "        i += 1\n",
    "\n",
    "    return snippets,new_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# from collections import defaultdict\n",
    "# with open('new_spans_labeled.json','r') as file:\n",
    "#     dataset=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultant={\"Cinematography\":[],\"Direction\":[],\"Story\":[],\"Characters\":[],\"Production Design\":[],\"Unique Concept\":[],\"Emotions\":[]}\n",
    "\n",
    "# def contains_alpha(s):\n",
    "#     return any(char.isalpha() for char in s)\n",
    "# for data in dataset:\n",
    "#     for key,value in data.items():\n",
    "#         if key!='review':\n",
    "#             if data[key]!=[]:\n",
    "#                 for item in data[key]:\n",
    "#                     if contains_alpha(item):\n",
    "#                         resultant[key].append(data)\n",
    "#                         break\n",
    "\n",
    "# resultant['Cinematography']=resultant['Cinematography'][:5000]\n",
    "\n",
    "# resultant['Direction']=resultant['Direction'][:5000]\n",
    "\n",
    "# resultant['Story']=resultant['Story'][:5000]\n",
    "\n",
    "# resultant['Characters']=resultant['Characters'][:5000]\n",
    "\n",
    "# resultant['Production Design']=resultant['Production Design'][:5000]\n",
    "\n",
    "# resultant['Unique Concept']=resultant['Unique Concept'][:5000]\n",
    "\n",
    "# resultant['Emotions']=resultant['Emotions'][:5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def labels_and_tokens(review, snippet,max_len=512):\n",
    "#     tokens = tokenizer.tokenize(review)\n",
    "#     token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "#     labels = [0] * len(token_ids)\n",
    "#     for snp in snippet:\n",
    "#         snippet_tokens = tokenizer.tokenize(snp)\n",
    "#         snippet_token_ids = tokenizer.convert_tokens_to_ids(snippet_tokens)\n",
    " \n",
    " \n",
    "#         for i in range(len(token_ids)):\n",
    "#             if token_ids[i:i+len(snippet_token_ids)] == snippet_token_ids:\n",
    "#                 labels[i:i+len(snippet_token_ids)] = [1] * len(snippet_token_ids)\n",
    "#                 break  # Assuming one occurrence of snippet in review\n",
    "       \n",
    "#         # Pad labels\n",
    "#         if len(labels) < max_len:\n",
    "#             labels = labels + [0] * (max_len - len(labels))\n",
    "#         else:\n",
    "#             labels = labels[:max_len]\n",
    " \n",
    "#     return labels,len(tokens)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set=[]\n",
    "# for result in resultant:\n",
    "#     for data in tqdm(resultant[result]):\n",
    "#         for key,value in data.items():\n",
    "#             if key!=\"review\":\n",
    "#                 labels,length_tokens=labels_and_tokens(data['review'],data[key])\n",
    "#                 if data[key]==[]:\n",
    "#                     test_set.append([data['review'],key,[],labels,0])\n",
    "#                 else:\n",
    "#                     test_set.append([data['review'],key,data[key],labels,length_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns=['review','aspect','snippets','labels','length']\n",
    "# test_set_save=pd.DataFrame(test_set,columns=columns)\n",
    "# test_set_save.to_csv('test_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset=pd.read_json('new_spans_labeled.json')\n",
    "dataset=dataset.sample(frac = 1,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_char(sentence1, sentence2):\n",
    "    set1 = set(sentence1.lower())\n",
    "    set2 = set(sentence2.lower())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cinematography 0.6117647892383771\n",
      "Direction 0.5890670131153023\n",
      "Story 0.7512071800118716\n",
      "Characters 0.7238620121034917\n",
      "Production Design 0.5069140887725108\n",
      "Unique Concept 0.17493709018535902\n",
      "Emotions 0.07063709478100669\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Suppress the warning\n",
    "logging.disable(logging.WARNING)\n",
    "cimematography=[]\n",
    "\n",
    "aspects = ['Cinematography', 'Direction', 'Story', 'Characters', \"Production Design\", \"Unique Concept\", \"Emotions\"]\n",
    "found=0\n",
    "not_found=0\n",
    "for aspect in aspects:\n",
    "    average_sim=[]\n",
    "\n",
    "    for i,(review,data) in enumerate(zip(dataset['review'],dataset[aspect])):\n",
    "        \n",
    "        if data!=[]:\n",
    "            # print(data)\n",
    "            snippets,new_predictions = predict_snippet(review,aspect,model,tokenizer)\n",
    "            # if len(snippets)>0:\n",
    "            #     print(snippets)\n",
    "            # print(data)\n",
    "            # print(snippets)\n",
    "            # print(\"***************************\")\n",
    "            # print(snippets)\n",
    "            smiliarities=[]\n",
    "            \n",
    "            for snippet in snippets:\n",
    "                similarity=0\n",
    "                for label in data:\n",
    "                    sim=jaccard_similarity_char(label, snippet)\n",
    "                    if sim > similarity:\n",
    "                        similarity=sim\n",
    "                smiliarities.append(similarity)\n",
    "            if len(smiliarities):\n",
    "                average_sim.append(sum(smiliarities)/len(smiliarities))\n",
    "            else:\n",
    "                average_sim.append(0)\n",
    "            found+=1\n",
    "            if found>500:\n",
    "                found=0\n",
    "                break\n",
    "            # print(\"==========================\")\n",
    "    print(aspect,sum(average_sim)/len(average_sim))\n",
    "print(not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
