{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "with open('../../new_spans_labeled.json', 'r') as file:\n",
        "    dataset = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize an empty list to store matching reviews\n",
        "reviews_list = []\n",
        "\n",
        "# Function to check if a snippet is present in the review and adjust its length if needed\n",
        "def find_snippet(review, snippet):\n",
        "    if snippet in review:\n",
        "        return True, snippet\n",
        "    # Check if the snippet, minus the last character, is in the review\n",
        "    elif snippet[:-1] in review:\n",
        "        return True, snippet[:-1]\n",
        "    return False, snippet\n",
        "\n",
        "# List of aspects to check in each review\n",
        "aspects = ['Cinematography', 'Direction', 'Story', 'Characters', \"Production Design\", \"Unique Concept\", \"Emotions\"]\n",
        "\n",
        "# Iterate over each data entry in the dataset\n",
        "for data in dataset:\n",
        "    review_text = data['review'].replace(\"'\", '').replace('\"', '').replace(\"“\",'').replace(' ]', '').lower().strip()\n",
        "    found = True\n",
        "\n",
        "    # Loop through each aspect and its corresponding items\n",
        "    for aspect in aspects:\n",
        "        for i, item in enumerate(data[aspect]):\n",
        "            snippet = item.replace(\"'\", '').replace('\"', '').replace(' ]', '').replace(\"“\",'').lower().strip()\n",
        "            is_found, adjusted_snippet = find_snippet(review_text, snippet)\n",
        "            if not is_found:\n",
        "                found = False\n",
        "                break\n",
        "            else:\n",
        "                # Update the original snippet to its adjusted version if necessary\n",
        "                data[aspect][i] = adjusted_snippet\n",
        "        if not found:\n",
        "            # print(snippet)\n",
        "            break\n",
        "\n",
        "    # If all snippets were found, add the data entry to reviews_list\n",
        "    if found:\n",
        "        reviews_list.append(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label counts:\n",
            "Cinematography: 7173\n",
            "Direction: 9625\n",
            "Story: 31194\n",
            "Characters: 26865\n",
            "Production Design: 6982\n",
            "Unique Concept: 9997\n",
            "Emotions: 9121\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Dictionary to hold counts\n",
        "label_counts = {\n",
        "    'Cinematography': 0,\n",
        "    'Direction': 0,\n",
        "    'Story': 0,\n",
        "    'Characters': 0,\n",
        "    'Production Design': 0,\n",
        "    'Unique Concept': 0,\n",
        "    'Emotions': 0\n",
        "}\n",
        "\n",
        "# Counting the number of elements for each label in each review\n",
        "for review in reviews_list:\n",
        "    for label in label_counts:\n",
        "        if len(review[label]):\n",
        "            # print(review[label])\n",
        "            label_counts[label] += 1\n",
        "\n",
        "# Printing the counts\n",
        "print(\"Label counts:\")\n",
        "for label, count in label_counts.items():\n",
        "    print(f\"{label}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "oUlztWyoDCs7",
        "outputId": "48057427-6b76-461f-971d-bbd27f6631e5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>aspect</th>\n",
              "      <th>snippets</th>\n",
              "      <th>aspect_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>Cinematography</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>Direction</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>Story</td>\n",
              "      <td>[the first thing that struck me about oz was i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>Characters</td>\n",
              "      <td>[em city is home to many..aryans, muslims, gan...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>Production Design</td>\n",
              "      <td>[]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242867</th>\n",
              "      <td>The first time I ever saw this movie was when ...</td>\n",
              "      <td>Story</td>\n",
              "      <td>[the only problem i have with this movie, howe...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242868</th>\n",
              "      <td>The first time I ever saw this movie was when ...</td>\n",
              "      <td>Characters</td>\n",
              "      <td>[the actors were amazing, treat williams is gr...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242869</th>\n",
              "      <td>The first time I ever saw this movie was when ...</td>\n",
              "      <td>Production Design</td>\n",
              "      <td>[]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242870</th>\n",
              "      <td>The first time I ever saw this movie was when ...</td>\n",
              "      <td>Unique Concept</td>\n",
              "      <td>[]</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242871</th>\n",
              "      <td>The first time I ever saw this movie was when ...</td>\n",
              "      <td>Emotions</td>\n",
              "      <td>[i remember loving it and everything about it....</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>242872 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review             aspect  \\\n",
              "0       One of the other reviewers has mentioned that ...     Cinematography   \n",
              "1       One of the other reviewers has mentioned that ...          Direction   \n",
              "2       One of the other reviewers has mentioned that ...              Story   \n",
              "3       One of the other reviewers has mentioned that ...         Characters   \n",
              "4       One of the other reviewers has mentioned that ...  Production Design   \n",
              "...                                                   ...                ...   \n",
              "242867  The first time I ever saw this movie was when ...              Story   \n",
              "242868  The first time I ever saw this movie was when ...         Characters   \n",
              "242869  The first time I ever saw this movie was when ...  Production Design   \n",
              "242870  The first time I ever saw this movie was when ...     Unique Concept   \n",
              "242871  The first time I ever saw this movie was when ...           Emotions   \n",
              "\n",
              "                                                 snippets  aspect_encoded  \n",
              "0                                                      []               0  \n",
              "1                                                      []               1  \n",
              "2       [the first thing that struck me about oz was i...               2  \n",
              "3       [em city is home to many..aryans, muslims, gan...               3  \n",
              "4                                                      []               4  \n",
              "...                                                   ...             ...  \n",
              "242867  [the only problem i have with this movie, howe...               2  \n",
              "242868  [the actors were amazing, treat williams is gr...               3  \n",
              "242869                                                 []               4  \n",
              "242870                                                 []               5  \n",
              "242871  [i remember loving it and everything about it....               6  \n",
              "\n",
              "[242872 rows x 4 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Aspects of interest\n",
        "aspects = ['Cinematography', 'Direction', 'Story', 'Characters', \"Production Design\", \"Unique Concept\", \"Emotions\"]\n",
        "\n",
        "# Aspect encoding mapping\n",
        "aspect_encoding = {aspect: index for index, aspect in enumerate(aspects)}\n",
        "\n",
        "# Prepare data for DataFrame\n",
        "data = []\n",
        "\n",
        "for review in reviews_list:\n",
        "    review_text = review[\"review\"]\n",
        "    for aspect in aspects:\n",
        "        snippets = review.get(aspect, [])  # Get the snippets for the aspect, default to an empty list\n",
        "        if not snippets:  # Ensure a row even if there are no snippets\n",
        "            snippets = [\"\"]  # Use an empty string for consistency\n",
        "\n",
        "        # Combine snippets into a single list for each aspect\n",
        "        entry = {\n",
        "            \"review\": review_text,\n",
        "            \"aspect\": aspect,\n",
        "            \"snippets\": [snippet.replace('\"','') for snippet in snippets],  # Store snippets as a list\n",
        "            \"aspect_encoded\": aspect_encoding[aspect]\n",
        "        }\n",
        "        data.append(entry)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "141915"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#count where snippets are zero:\n",
        "count = 0\n",
        "for i in df['snippets']:\n",
        "    # print(i)\n",
        "    if i == ['']:\n",
        "        count += 1\n",
        "\n",
        "count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(\"../data/formatted_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VoD5soblDCs-"
      },
      "outputs": [],
      "source": [
        "new_rows=[]\n",
        "for index, row in df.iterrows():\n",
        "    snippets=[]\n",
        "    not_found=False\n",
        "    for snippet in row['snippets']:\n",
        "        if snippet!='':\n",
        "            if snippet.lower() in row['review'].lower():\n",
        "                snippets.append(snippet.lower())\n",
        "            else:\n",
        "                # print(snippet)\n",
        "                not_found=True\n",
        "                break\n",
        "    if not not_found:\n",
        "        new_rows.append([row['review'].lower(),row['aspect'],snippets])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fN6EL6G-DCs_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wisalkhanmv/anaconda3/envs/lowerated/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-08-18 00:42:07.114551: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-08-18 00:42:07.193805: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-18 00:42:07.229675: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-18 00:42:07.239549: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-18 00:42:07.298351: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-18 00:42:08.299928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dC7Pxae9DCs_"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(new_rows, columns = ['review', 'aspect','snippet'])\n",
        "aspect_encoder = LabelEncoder()\n",
        "df['aspect_encoded'] = aspect_encoder.fit_transform(df['aspect'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-SQc2hjNf6b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "aspect\n",
              "Cinematography       33169\n",
              "Production Design    32917\n",
              "Direction            31615\n",
              "Unique Concept       31163\n",
              "Emotions             30956\n",
              "Characters           23184\n",
              "Story                19840\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aspect_counts = df['aspect'].value_counts()\n",
        "aspect_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T48YD2gsJkT2",
        "outputId": "c8b56159-6b5f-4984-852c-a6e65fcd5775"
      },
      "outputs": [],
      "source": [
        "# Define the dataset class\n",
        "class ReviewAspectDataset(Dataset):\n",
        "    def __init__(self, reviews, aspects, snippets, tokenizer, max_len):\n",
        "        self.reviews = reviews\n",
        "        self.aspects = aspects\n",
        "        self.snippets = snippets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = str(self.reviews[idx])\n",
        "        aspect = str(self.aspects[idx])\n",
        "        snippets = self.snippets[idx]\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            aspect,\n",
        "            add_special_tokens=False,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        tokens = self.tokenizer.tokenize(review)\n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        labels = [0] * len(token_ids)\n",
        "\n",
        "        for snippet in snippets:\n",
        "            snippet_tokens = self.tokenizer.tokenize(snippet)\n",
        "            snippet_token_ids = self.tokenizer.convert_tokens_to_ids(snippet_tokens)\n",
        "\n",
        "            for i in range(len(token_ids) - len(snippet_token_ids) + 1):\n",
        "                if token_ids[i:i+len(snippet_token_ids)] == snippet_token_ids:\n",
        "                    labels[i:i+len(snippet_token_ids)] = [1] * len(snippet_token_ids)\n",
        "                    break  # Assuming one occurrence of snippet in review\n",
        "\n",
        "        # Pad or truncate labels to match max_len\n",
        "        if len(labels) < self.max_len:\n",
        "            labels += [0] * (self.max_len - len(labels))\n",
        "        else:\n",
        "            labels = labels[:self.max_len]\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.long)\n",
        "        }\n",
        "# Tokenizer and dataset preparation\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_len = 512\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oaQ1FWHnJl9N"
      },
      "outputs": [],
      "source": [
        "train_dataset = ReviewAspectDataset(\n",
        "    reviews=train_df['review'].to_numpy(),\n",
        "    aspects=train_df['aspect'].to_numpy(),\n",
        "    snippets=train_df['snippet'].to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        ")\n",
        "\n",
        "val_dataset = ReviewAspectDataset(\n",
        "    reviews=val_df['review'].to_numpy(),\n",
        "    aspects=val_df['aspect'].to_numpy(),\n",
        "    snippets=val_df['snippet'].to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hPrBikRGJsr8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/wisalkhanmv/anaconda3/envs/lowerated/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "# Define the model\n",
        "import os\n",
        "from transformers import EarlyStoppingCallback\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    load_best_model_at_end=True,  # To ensure the best model is loaded at the end\n",
        "    save_total_limit=1,  # To keep only the best model\n",
        "    learning_rate= 5e-05\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q_YOZxWxJ0vN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 500/81140 [11:02<30:11:08,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0939, 'grad_norm': 0.5681623816490173, 'learning_rate': 5e-05, 'epoch': 0.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                      \n",
            "  1%|          | 500/81140 [49:15<30:11:08,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.05377301946282387, 'eval_runtime': 2293.0331, 'eval_samples_per_second': 17.692, 'eval_steps_per_second': 2.212, 'epoch': 0.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 1000/81140 [56:18<18:56:10,  1.18it/s]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0527, 'grad_norm': 0.2634539306163788, 'learning_rate': 4.968998015873016e-05, 'epoch': 0.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                       \n",
            "  1%|          | 1000/81140 [1:18:19<18:56:10,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.049038052558898926, 'eval_runtime': 1320.6064, 'eval_samples_per_second': 30.72, 'eval_steps_per_second': 3.841, 'epoch': 0.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1500/81140 [1:25:22<18:36:36,  1.19it/s]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0493, 'grad_norm': 0.47434914112091064, 'learning_rate': 4.937996031746032e-05, 'epoch': 0.07}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  2%|▏         | 1500/81140 [1:47:25<18:36:36,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.04514707997441292, 'eval_runtime': 1323.4653, 'eval_samples_per_second': 30.654, 'eval_steps_per_second': 3.832, 'epoch': 0.07}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2000/81140 [1:54:27<18:19:47,  1.20it/s]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0416, 'grad_norm': 0.41527578234672546, 'learning_rate': 4.9069940476190476e-05, 'epoch': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  2%|▏         | 2000/81140 [2:16:10<18:19:47,  1.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.041868217289447784, 'eval_runtime': 1303.3821, 'eval_samples_per_second': 31.126, 'eval_steps_per_second': 3.891, 'epoch': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 2500/81140 [2:23:11<18:14:56,  1.20it/s]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0418, 'grad_norm': 0.34335920214653015, 'learning_rate': 4.875992063492064e-05, 'epoch': 0.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  3%|▎         | 2500/81140 [2:44:54<18:14:56,  1.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03882870450615883, 'eval_runtime': 1303.3097, 'eval_samples_per_second': 31.128, 'eval_steps_per_second': 3.892, 'epoch': 0.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▎         | 3000/81140 [2:51:54<18:14:20,  1.19it/s]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0414, 'grad_norm': 0.2850511372089386, 'learning_rate': 4.84499007936508e-05, 'epoch': 0.15}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  4%|▎         | 3000/81140 [3:13:37<18:14:20,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.04144946113228798, 'eval_runtime': 1303.1376, 'eval_samples_per_second': 31.132, 'eval_steps_per_second': 3.892, 'epoch': 0.15}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 3500/81140 [3:20:38<18:11:54,  1.19it/s]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0396, 'grad_norm': 0.36550506949424744, 'learning_rate': 4.813988095238096e-05, 'epoch': 0.17}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            "  4%|▍         | 3500/81140 [3:42:19<18:11:54,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.04070288687944412, 'eval_runtime': 1301.334, 'eval_samples_per_second': 31.175, 'eval_steps_per_second': 3.898, 'epoch': 0.17}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 3500/81140 [3:42:20<82:12:13,  3.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 13340.6505, 'train_samples_per_second': 48.656, 'train_steps_per_second': 6.082, 'train_loss': 0.051476426805768694, 'epoch': 0.17}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3500, training_loss=0.051476426805768694, metrics={'train_runtime': 13340.6505, 'train_samples_per_second': 48.656, 'train_steps_per_second': 6.082, 'total_flos': 7316309188608000.0, 'train_loss': 0.051476426805768694, 'epoch': 0.1725412866650234})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import logging\n",
        "logging.disable(logging.WARNING)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cmqSX7JfgqY8"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uvl_85K4gx0H"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5072/5072 [22:14<00:00,  3.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           O     0.9895    0.9985    0.9940  20479523\n",
            "   B-SNIPPET     0.7091    0.2548    0.3749    291805\n",
            "\n",
            "    accuracy                         0.9881  20771328\n",
            "   macro avg     0.8493    0.6266    0.6844  20771328\n",
            "weighted avg     0.9855    0.9881    0.9853  20771328\n",
            "\n",
            "Accuracy: 0.9881\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.03882870450615883,\n",
              " 'eval_model_preparation_time': 0.0028,\n",
              " 'eval_accuracy': 0.9880620536154453,\n",
              " 'eval_f1': 0.68445,\n",
              " 'eval_precision': 0.62665,\n",
              " 'eval_recall': 0.8492999999999999,\n",
              " 'eval_runtime': 1334.6682,\n",
              " 'eval_samples_per_second': 30.396,\n",
              " 'eval_steps_per_second': 3.8}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "# Custom evaluation function to generate classification report\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=2)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    # Flatten the predictions and labels\n",
        "    preds_flat = preds.flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    # Filter out the labels where the label is -100 (ignore index)\n",
        "    mask = labels_flat != -100\n",
        "    preds_flat = preds_flat[mask]\n",
        "    labels_flat = labels_flat[mask]\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(labels_flat, preds_flat, target_names=['O', 'B-SNIPPET'], digits=4)\n",
        "    accuracy = accuracy_score(labels_flat, preds_flat)\n",
        "\n",
        "    print(\"\\nClassification Report:\\n\", report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Safely extract metrics from the report\n",
        "    report_lines = report.split('\\n')\n",
        "    f1_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    \n",
        "    # Extract metrics from each class, skipping the header and footer lines\n",
        "    for line in report_lines[2:-3]:\n",
        "        parts = line.split()\n",
        "        if len(parts) >= 4:  # Ensure that the line has enough parts\n",
        "            recall_scores.append(float(parts[-4]))\n",
        "            precision_scores.append(float(parts[-3]))\n",
        "            f1_scores.append(float(parts[-2]))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": np.mean(f1_scores) if f1_scores else 0.0,\n",
        "        \"precision\": np.mean(precision_scores) if precision_scores else 0.0,\n",
        "        \"recall\": np.mean(recall_scores) if recall_scores else 0.0\n",
        "    }\n",
        "\n",
        "# Trainer with custom metrics\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics  # Adding the custom compute_metrics function\n",
        ")\n",
        "\n",
        "# Evaluate the model on validation dataset\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8vSEwLXfeHZ2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./spans_based_bert_model/tokenizer_config.json',\n",
              " './spans_based_bert_model/special_tokens_map.json',\n",
              " './spans_based_bert_model/vocab.txt',\n",
              " './spans_based_bert_model/added_tokens.json')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Save the model\n",
        "model.save_pretrained('./spans_based_bert_model')\n",
        "tokenizer.save_pretrained('./spans_based_bert_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2Ram0_fpeIkC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "\n",
        "import re\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = BertForTokenClassification.from_pretrained('./spans_based_bert_model')\n",
        "tokenizer = BertTokenizer.from_pretrained('./spans_based_bert_model')\n",
        "\n",
        "def remove_specific_characters(strings_list):\n",
        "    # Define the characters to be removed\n",
        "    characters_to_remove = {\n",
        "    '\\x8d', '\\x8b', '\\x8c', '\\x8f', '\\x87', '\\x8e', '\\x81',\n",
        "    '\\x8a', '\\x83', '\\x94', '\\x95', '\\x97', '\\x91', '\\x89',\n",
        "    '\\x80', '\\x99', '\\x9e', '\\xad', '\\x9d', '\\x98', '\\x93',\n",
        "    '\\x82', '\\x9c', '\\x9f'\"®\", \"´\", \"¿\", \"¥\",\n",
        "        \"\\u00c3\", \"\\u00a2\", \"\\u00c2\", \"\\u0080\", \"\\u00c2\", \"\\u0099\"\n",
        "    }\n",
        "\n",
        "    cleaned_strings_list = []\n",
        "\n",
        "    for string in strings_list:\n",
        "        cleaned_string = ''.join(char for char in string if char not in characters_to_remove)\n",
        "        cleaned_strings_list.append(cleaned_string)\n",
        "\n",
        "    return cleaned_strings_list\n",
        "\n",
        "def remove_double_spaces(strings):\n",
        "    pattern = re.compile(r'\\s{2,}')  # Regex to match two or more spaces\n",
        "    return [pattern.sub(' ', text) for text in strings]\n",
        "\n",
        "def remove_multiple_punctuation(strings):\n",
        "    # Create patterns to find multiple occurrences of ., !, and ,\n",
        "    patterns = {\n",
        "        r'\\.{2,}': '.',\n",
        "        r'\\!{2,}': '!',\n",
        "        r'\\,{2,}': ','\n",
        "    }\n",
        "\n",
        "    # Process each string in the list\n",
        "    cleaned_strings = []\n",
        "    for text in strings:\n",
        "        for pattern, replacement in patterns.items():\n",
        "            text = re.sub(pattern, replacement, text)\n",
        "        cleaned_strings.append(text)\n",
        "\n",
        "    return cleaned_strings\n",
        "\n",
        "\n",
        "\n",
        "def predict_snippet(review, aspect, model, tokenizer, max_len=256):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        review,\n",
        "        aspect,\n",
        "        add_special_tokens=False,\n",
        "        max_length=max_len,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation='longest_first'\n",
        "    )\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Make predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=2).flatten().tolist()\n",
        "    new_predictions=predictions.copy()\n",
        "    # Decode the tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten().tolist())\n",
        "    # print(tokens)\n",
        "    snippets=[]\n",
        "    snippet=[]\n",
        "    i = 0\n",
        "    for token, label in zip(tokens, predictions):\n",
        "        if label == 1:\n",
        "            new_predictions[i] = 1\n",
        "            snippet.append(token)\n",
        "        elif label == 0 and i > 0 and i + 1 < len(tokens) and predictions[i - 1] == 1 and predictions[i + 1] == 1:\n",
        "            new_predictions[i] = 1\n",
        "            snippet.append(token)\n",
        "        elif len(snippet):\n",
        "            snippets.append(' '.join(snippet))\n",
        "            snippet = []\n",
        "        i += 1\n",
        "\n",
        "    for i in range(1, len(new_predictions) - 2):\n",
        "        # Check for the pattern 1,0,0,1\n",
        "        if new_predictions[i] == 0 and new_predictions[i+1] == 0 and new_predictions[i-1] == 1 and new_predictions[i+2] == 1:\n",
        "            new_predictions[i] = 1\n",
        "            new_predictions[i+1] = 1\n",
        "\n",
        "    # print(snippets)\n",
        "    return snippets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Rj072SAUeNFi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original review:  the story was amazing but the cinematography wasn't it\n",
            "Cinematography []\n",
            "\n",
            "-------------------\n",
            "Direction []\n",
            "\n",
            "-------------------\n",
            "Story []\n",
            "\n",
            "-------------------\n",
            "Characters []\n",
            "\n",
            "-------------------\n",
            "Production Design []\n",
            "\n",
            "-------------------\n",
            "Unique Concept []\n",
            "\n",
            "-------------------\n",
            "Emotions []\n",
            "\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    original_review= remove_double_spaces([text])\n",
        "    original_review= remove_multiple_punctuation(original_review)\n",
        "    original_review = remove_specific_characters(original_review)[0]\n",
        "    text = original_review.lower()\n",
        "    text = re.sub(r'\\n+', ' ', text)  # Replace newlines with a space\n",
        "    text = re.sub(r'\\.\\.+', '.', text)  # Replace multiple periods with a single period\n",
        "    text=text.replace(',','')\n",
        "    text=text.replace('.','')\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces with a single space\n",
        "    return text\n",
        "\n",
        "def fix_special_characters(snippet):\n",
        "    snippet=snippet.replace(\"[UNK]\",'')\n",
        "    snippet=snippet.replace(\" ##\",'')\n",
        "    snippet=snippet.replace(\" '\",\"'\")\n",
        "    snippet=snippet.replace(\" ’\",\"’\")\n",
        "    snippet=snippet.replace(\"’ \",\"’\")\n",
        "    snippet=snippet.replace(\"' \",\"'\")\n",
        "    snippet=snippet.replace(\" -\",\"-\")\n",
        "    snippet=snippet.replace(\"- \",\"-\")\n",
        "    snippet=snippet.replace(\"/ \",\"/\")\n",
        "    snippet=snippet.replace(\" /\",\"/\")\n",
        "    snippet=snippet.replace(\" :\",\":\")\n",
        "    snippet=snippet.replace(\": \",\":\")\n",
        "    return snippet\n",
        "\n",
        "aspects=['Cinematography', 'Direction', 'Story', 'Characters', \"Production Design\", \"Unique Concept\", \"Emotions\"]\n",
        "original_review=\"\"\"The story was amazing but the cinematography wasn't it.\"\"\"\n",
        "\n",
        "#     print(aspect, new_snippets)\n",
        "original_review=clean_text(original_review)\n",
        "\n",
        "print(\"original review: \",original_review)\n",
        "\n",
        "for aspect in aspects:\n",
        "    predicted_snippets = predict_snippet(original_review, aspect, model, tokenizer)\n",
        "\n",
        "    new_snippets = []\n",
        "    for snippet in predicted_snippets:\n",
        "        new_snippets.append(fix_special_characters(snippet))\n",
        "\n",
        "    print(aspect, new_snippets, end='\\n')\n",
        "    print(\"\\n-------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sequence': \"the cinematography wasn't great\", 'labels': ['cinematography negative', 'cinematography positive'], 'scores': [0.9993323683738708, 0.0006676383200101554]}\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\")\n",
        "\n",
        "# Example snippet\n",
        "snippet = \"the cinematography wasn't great\"\n",
        "candidate_labels = [\"cinematography positive\", \"cinematography negative\"]\n",
        "\n",
        "# Classify the sentiment\n",
        "sentiment_result = classifier(snippet, candidate_labels)\n",
        "print(sentiment_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
