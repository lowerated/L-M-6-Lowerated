{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def clean_span(span):\n",
    "    \"\"\"\n",
    "    Cleans the given span string by ensuring balanced quotes and removing unwanted commas.\n",
    "\n",
    "    Args:\n",
    "        span (str): The span string to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned span string.\n",
    "    \"\"\"\n",
    "    span = span.strip()\n",
    "    if span.endswith('\"],') or span.endswith('\"]'):\n",
    "        span = span.rstrip('\"],').rstrip('\"]') + '\"'\n",
    "    return span\n",
    "\n",
    "def parse_attribute_spans(response_str):\n",
    "    \"\"\"\n",
    "    Parses the attribute spans from the given string and returns a dictionary.\n",
    "\n",
    "    Args:\n",
    "        response_str (str): The response string containing the attribute spans.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with attributes as keys and lists of strings as values.\n",
    "    \"\"\"\n",
    "    attributes = [\"Cinematography\", \"Direction\", \"Story\", \"Characters\", \"Production Design\", \"Unique Concept\", \"Emotions\"]\n",
    "    spans = {attr: [] for attr in attributes}\n",
    "\n",
    "    # Remove the surrounding curly braces and split the response into lines\n",
    "    lines = response_str.strip()[1:-1].strip().splitlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if \":\" in line:\n",
    "            parts = line.split(\":\", 1)\n",
    "            attr = parts[0].strip()\n",
    "            if attr in attributes:\n",
    "                # Remove the surrounding square brackets and split by comma\n",
    "                span_list_str = parts[1].strip().strip(\"[]\").strip()\n",
    "                if span_list_str:\n",
    "                    # Split by comma but handle cases where there might be commas within strings\n",
    "                    spans[attr] = [clean_span(item.strip().strip('\"').strip()) for item in span_list_str.split('\", ') if item.strip().strip('\"').strip() and item != ',']\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9516</th>\n",
       "      <td>This isn't \"so bad it's good\"--It's \"so bad, i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9517</th>\n",
       "      <td>I'm a fan of the horror movie, regardless of w...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9518</th>\n",
       "      <td>maybe i need to have my head examined,but i th...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9519</th>\n",
       "      <td>Larry Buchanan. Yep, same guy who did \"Attack ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9520</th>\n",
       "      <td>Ritchie's first two films were snappy, stylish...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40082 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "9516   This isn't \"so bad it's good\"--It's \"so bad, i...  negative\n",
       "9517   I'm a fan of the horror movie, regardless of w...  negative\n",
       "9518   maybe i need to have my head examined,but i th...  positive\n",
       "9519   Larry Buchanan. Yep, same guy who did \"Attack ...  negative\n",
       "9520   Ritchie's first two films were snappy, stylish...  negative\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[40082 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Labeling Dataset\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "import os, json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "apikey = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=apikey)\n",
    "\n",
    "df = pd.read_csv(\"../data/IMDB Dataset.csv\")\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df = df[9500:]\n",
    "reviews = list(df[\"review\"][9500:])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Request to GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "import os, json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "apikey = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=apikey)\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "You are an assistant who gives specific attributes. The attributes are Cinematography, Direction, Story, Characters, Production Design, Unique Concept, and Emotions. \n",
    "\n",
    "YOUR INPUT WOULD BE LIKE THIS:\n",
    "Review: \"The cinematography was stunning, but the story was weak. I loved the movie. There wasn't anything unique in the movie. characters could've been better tho.\"\n",
    "\n",
    "YOU MUST FOLLOW THE OUTPUT FORMAT GIVEN BELOW. DON'T WRITE ANYTHING ELSE:\n",
    "{\n",
    "Cinematography: [list of strings with chunks where cinematography is discussed],\n",
    "Direction: [list of strings with chunks where direction is discussed],\n",
    "Story: [list of strings with chunks where story is discussed],\n",
    "Characters: [list of strings with chunks where characters are discussed],\n",
    "Production Design: [list of strings with chunks where production design is discussed],\n",
    "Unique Concept: [list of strings with chunks where unique concept is discussed],\n",
    "Other: [list of strings that mentions other things related to movie]\n",
    "}\n",
    "\n",
    "if something is not discussed, add empty list infront of it.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define the attributes\n",
    "attributes = [\"Cinematography\", \"Direction\", \"Story\", \"Characters\", \"Production Design\", \"Unique Concept\", \"Emotions\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_sentiment_spans(review):\n",
    "    prompt = \"Label the following review below:\" + f\"\\nReview: {review}\\n\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content.strip()\n",
    "    spans = parse_attribute_spans(response)\n",
    "    return spans\n",
    "\n",
    "def parse_attribute_spans(response):\n",
    "    try:\n",
    "        spans = json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        spans = {attr: [] for attr in attributes}\n",
    "    return spans\n",
    "\n",
    "def label_dataset_and_save(sentences, batch_size, output_file, checkpoint_file):\n",
    "    start_index = 0\n",
    "\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as file:\n",
    "            start_index = int(file.read().strip())\n",
    "\n",
    "    requests = []\n",
    "    for i in range(start_index, len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        prompt = f\"Label the following review below:\\nReview: {sentence}\\n\"\n",
    "        request = {\n",
    "            \"custom_id\": f\"request-{i+1}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"max_tokens\": 1000\n",
    "            }\n",
    "        }\n",
    "        requests.append(request)\n",
    "\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(sentences):\n",
    "            with open(output_file, 'a') as file:\n",
    "                for req in requests:\n",
    "                    json.dump(req, file)\n",
    "                    file.write('\\n')\n",
    "            requests = []\n",
    "            with open(checkpoint_file, 'w') as file:\n",
    "                file.write(str(i + 1))\n",
    "            print(f\"Processed and saved {i + 1} sentences\")\n",
    "\n",
    "\n",
    "output_file = '../data/spans_batch_requests.jsonl'\n",
    "checkpoint_file = 'checkpoint.txt'\n",
    "batch_size = 40082  # Adjust batch size as needed\n",
    "\n",
    "label_dataset_and_save(reviews, batch_size, output_file, checkpoint_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Tokens (Approx) per request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens used in the file: 18926661\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import jsonlines\n",
    "\n",
    "\n",
    "# Initialize the encoding for GPT-4\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Open the .jsonl file\n",
    "with jsonlines.open('../data/spans_batch_requests.jsonl') as reader:\n",
    "    total_tokens = 0\n",
    "    for obj in reader:\n",
    "        # Assuming each object has 'review' and 'label' keys\n",
    "        example_sentence = f\"\"\"{obj}\"\"\"\n",
    "\n",
    "        # Combine the prompt and the example sentence\n",
    "        full_prompt = example_sentence\n",
    "\n",
    "        # Encode the text to get the number of tokens\n",
    "        num_tokens = len(encoding.encode(full_prompt))\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "print(f\"Total number of tokens used in the file: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Uploading Your Batch Input File\n",
    "Similar to our Fine-tuning API, you must first upload your input file so that you can reference it correctly when kicking off batches. Upload your .jsonl file using the Files API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Batch\n",
    "Once you've successfully uploaded your input file, you can use the input File object's ID to create a batch. In this case, let's assume the file ID is file-abc123. For now, the completion window can only be set to 24h. You can also provide custom metadata via an optional metadata parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "batch_input_file = client.files.create(\n",
    "  file=open(\"../data/spans_batch_requests.jsonl\", \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "\n",
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"lm6 spans job\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor and Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.batches.retrieve(\"batch_Bd91ong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.cancel(\"batch_Bd91ong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "file_response = client.files.content(\"file-xyz123\")\n",
    "print(file_response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lowerated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
